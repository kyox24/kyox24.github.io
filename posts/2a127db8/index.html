<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>PyTorch中nn.Embedding相关备忘 | Foggy World</title><meta name="keywords" content="PyTorch"><meta name="author" content="Kyox24"><meta name="copyright" content="Kyox24"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="最近在学习PyTorch，本文主要记录关于nn.Embedding的使用与理解。">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch中nn.Embedding相关备忘">
<meta property="og:url" content="https://foggyworld.life/posts/2a127db8/index.html">
<meta property="og:site_name" content="Foggy World">
<meta property="og:description" content="最近在学习PyTorch，本文主要记录关于nn.Embedding的使用与理解。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/kyox24/PicBed/MEMO_PY_EM.png">
<meta property="article:published_time" content="2021-05-11T14:54:03.000Z">
<meta property="article:modified_time" content="2021-05-12T08:17:20.923Z">
<meta property="article:author" content="Kyox24">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/kyox24/PicBed/MEMO_PY_EM.png"><link rel="shortcut icon" href="/images/GAME%20CONSOLE.png"><link rel="canonical" href="https://foggyworld.life/posts/2a127db8/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-05-12 17:17:20'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper/swiper-bundle.min.css"><link rel="stylesheet" href="/style/showbb_in_index.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" data-lazy-src="https://cdn.jsdelivr.net/gh/kyox24/PicBed/头像.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/bb/"><i class="fa-fw fas fa-quote-left"></i><span> 自言自语</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Foggy World</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/bb/"><i class="fa-fw fas fa-quote-left"></i><span> 自言自语</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">PyTorch中nn.Embedding相关备忘</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-05-11T14:54:03.000Z" title="发表于 2021-05-11 23:54:03">2021-05-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-05-12T08:17:20.923Z" title="更新于 2021-05-12 17:17:20">2021-05-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%87%E5%BF%98%E8%AE%B0%E5%BD%95/">备忘记录</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="PyTorch中nn.Embedding相关备忘"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div><article class="post-content" id="article-container"><h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h2>
<hr />
<p>最近在学习<code>PyTorch</code>，本文主要记录关于<code>nn.Embedding</code>的使用与理解。</p>
<h2 id="什么是embedding"><a class="markdownIt-Anchor" href="#什么是embedding"></a> 什么是Embedding</h2>
<hr />
<p>Embedding可以直接译作嵌入，在机器学习中以笔者目前的水平，最常听到的关于Embedding的词有：</p>
<ul>
<li><code>Embedding Network</code>，意思即为在神经网络的主体架构之外嵌入了一个相对独立的神经网络，比如说<code>CESAGAN</code>中就使用一个<code>Embedding Network</code>来学习关于游戏特征（如怪物数量等）的向量表达而不是使用最朴素的<code>one-hot编码</code>来表达游戏特征，这么做的好处有两点：
<ol>
<li>在一般情况下，使用<code>Embedding Network</code>可以节省计算所需资源。</li>
<li><code>Embedding Network</code>可以减少信息丢失，在我们使用<code>one-hot编码</code>来进行表达时，蕴含着你要表达的所以特征之间都有相同的关联程度（或者不关联程度），例如你可以用<code>one-hot编码</code>的<code>(1, 0, 0)</code>表示法师、<code>(0, 1, 0)</code>表示战士，但是用<code>(0, 0, 1)</code>表示术士的话在一些任务中可能就天然地丢失了一些信息，比如从攻击方式的角度考虑，术士和法师的相关程度应该更高一些，用<code>Embedding Network</code>来训练一个表达方式就有可能缓解这个问题。</li>
</ol>
</li>
<li><code>Word Embedding</code>，主要出现在NLP的相关研究中，以我个人的理解，<code>Word Embedding</code>就是继承于上面说的第二个思想，即找到一个恰当的表达各个单词之间相关的程度的向量表示。</li>
</ul>
<p>所以实际上，我们可以认为Embedding做的事就是为每一个「原数据」（例如一个单词）建立了一个映射，把它们映射到「能表达原数据之间相关性的向量表示」，如下：</p>
<img src= "/images/Ripple.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/kyox24/PicBed/py-em.png" style="zoom:50%;" />
<div class="note info flat"><p>以下主要来自<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/58718612/what-exactly-happens-inside-embedding-layer-in-pytorch">What “exactly” happens inside embedding layer in pytorch?</a></p>
</div>
<h2 id="pytorch中nnembedding的用法"><a class="markdownIt-Anchor" href="#pytorch中nnembedding的用法"></a> PyTorch中<code>nn.Embedding</code>的用法</h2>
<hr />
<p>假设现在有一个只有10种单词的语言，你显然可以使用<code>[0, 1, 2, 3,..., 9]</code>的<code>token</code>来表示这个语言中的任何一个句子。此时我们需要对某个已经<code>token</code>化的句子<code>[1, 5, 9]</code>做个50维的Embedding，则在<code>PyTorch</code>中如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">tokens = torch.tensor([<span class="number">1</span>,<span class="number">5</span>,<span class="number">9</span>], dtype=torch.long)</span><br><span class="line">embedding = nn.Embedding(num_embeddings=<span class="number">10</span>, embedding_dim=<span class="number">50</span>)</span><br><span class="line">embedded_words = embedding(tokens)</span><br></pre></td></tr></table></figure>
<h2 id="pytorch中使用了nnembedding之后训练时发生了什么"><a class="markdownIt-Anchor" href="#pytorch中使用了nnembedding之后训练时发生了什么"></a> PyTorch中使用了<code>nn.Embedding</code>之后训练时发生了什么</h2>
<hr />
<p>实际上就跟一个<code>one-hot编码</code>+一个<code>Linear Layer</code>（无bias）一样，在训练时会计算这个<code>Embedding Layer</code>（或者说<code>Linear Layer</code>）的梯度，并进行参数的更新。</p>
<p>关于上述结论的实验，先看看使用<code>nn.Embedding</code>时的输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tokens = torch.tensor([<span class="number">1</span>,<span class="number">5</span>,<span class="number">9</span>], dtype=torch.long)</span><br><span class="line">embedding = nn.Embedding(num_embeddings=<span class="number">10</span>, embedding_dim=<span class="number">50</span>)</span><br><span class="line">embedded_words = embedding(tokens)</span><br><span class="line"></span><br><span class="line">embedded_words</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.5485, -1.5885,  0.5281,  0.8964,  0.9975, -0.2156,  1.3266, -0.1086,</span><br><span class="line">         -1.0265,  0.0436, -0.9724,  0.2803,  0.5700,  1.4841, -1.4556,  0.5582,</span><br><span class="line">         -0.5062,  0.4655, -0.8604, -0.8528,  2.0163,  0.1693, -0.9030, -1.7102,</span><br><span class="line">         -0.1362,  1.4530,  0.5620,  1.1591, -1.7210, -0.0590,  0.8455, -0.1304,</span><br><span class="line">         -0.3232, -0.7119, -0.2273, -2.5322,  0.3415,  1.1048,  0.0957, -0.0299,</span><br><span class="line">          0.8553, -0.0249,  2.2696,  0.8602, -1.6499, -0.0435,  0.4498, -0.5506,</span><br><span class="line">         -0.8203,  1.1264],</span><br><span class="line">        [ 0.2495, -0.5313, -0.6396, -0.4482, -0.2441,  0.1281, -1.3671,  0.7507,</span><br><span class="line">          0.5275, -0.3547, -0.6097,  1.3632, -0.3903,  1.4880,  0.2661,  0.1100,</span><br><span class="line">         -1.4546,  1.0616,  0.3370, -1.9703,  1.1549,  1.5273, -0.7556,  1.1663,</span><br><span class="line">         -0.1252, -0.8650,  1.3109, -0.0671, -1.1961, -0.5204,  0.2742, -0.4896,</span><br><span class="line">         -0.1334, -1.1338, -0.4657, -0.1748,  1.1600, -1.3987,  0.9809, -0.8172,</span><br><span class="line">          1.2058, -0.2068,  0.0058, -0.1539, -0.4198,  0.9184,  0.7068,  0.1542,</span><br><span class="line">         -0.1607, -0.2178],</span><br><span class="line">        [-0.0893,  0.4042, -0.7877, -0.0397, -0.3084,  0.3697,  0.2694, -0.6996,</span><br><span class="line">          0.3646,  1.0584,  1.7095, -0.2206,  0.1061, -0.5512,  1.2652, -0.6175,</span><br><span class="line">          0.2025, -0.1552, -0.0896, -0.8754,  0.6843, -0.2502,  0.4087, -0.7957,</span><br><span class="line">         -0.4200,  1.1480,  0.5463, -0.2346,  0.5102,  1.0036,  0.0231, -0.7988,</span><br><span class="line">          2.1893, -0.5216, -0.4245, -0.2605,  0.3182,  0.2125, -0.4399, -1.1013,</span><br><span class="line">         -1.2728, -2.4673, -1.3860,  0.0700, -0.4738,  0.1426, -0.1598,  0.5959,</span><br><span class="line">          0.9661,  1.0521]], grad_fn=&lt;EmbeddingBackward&gt;)</span><br></pre></td></tr></table></figure>
<p>用无bias的<code>nn.Linear</code>复现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">toens_onehot = F.one_hot(tokens, num_classes=<span class="number">10</span>).<span class="built_in">float</span>()</span><br><span class="line">FC = nn.Linear(<span class="number">10</span>, <span class="number">50</span>, bias=<span class="literal">False</span>)</span><br><span class="line">embedded_words_FC = FC(toens_onehot)</span><br><span class="line"></span><br><span class="line">embedded_words_FC</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.0085,  0.1786,  0.1411, -0.2134,  0.2197,  0.1373, -0.2857, -0.3061,</span><br><span class="line">         -0.3106,  0.0035,  0.2999, -0.0387,  0.1224,  0.2741,  0.2068,  0.2743,</span><br><span class="line">          0.1008,  0.0332,  0.3073, -0.2061, -0.0360,  0.1797,  0.2097,  0.2394,</span><br><span class="line">          0.2647,  0.0481,  0.2804, -0.2359,  0.2762, -0.2883, -0.1496, -0.1187,</span><br><span class="line">         -0.1897,  0.1400,  0.0275, -0.1759, -0.0353,  0.1569, -0.1848,  0.2898,</span><br><span class="line">         -0.3150, -0.1627, -0.2669, -0.2567,  0.0860, -0.0167,  0.1402, -0.0542,</span><br><span class="line">          0.2868,  0.0964],</span><br><span class="line">        [ 0.1224,  0.0015, -0.2786, -0.2680,  0.0404,  0.1108,  0.1908, -0.1893,</span><br><span class="line">          0.2985,  0.3123, -0.0704,  0.3089, -0.0223, -0.1332, -0.0999, -0.1303,</span><br><span class="line">          0.0254,  0.1377, -0.1556,  0.0495,  0.1962, -0.2426, -0.2460,  0.2970,</span><br><span class="line">         -0.1528, -0.3053, -0.3016,  0.0874,  0.1212,  0.1545,  0.1958,  0.0982,</span><br><span class="line">          0.0533,  0.1192,  0.2085,  0.2223, -0.0982,  0.1692,  0.0965, -0.0677,</span><br><span class="line">         -0.0845,  0.2640,  0.2296, -0.2635,  0.1993, -0.1912, -0.0646, -0.1446,</span><br><span class="line">         -0.1705, -0.2825],</span><br><span class="line">        [ 0.2687,  0.1678,  0.0823, -0.0391, -0.1992, -0.0028,  0.0916,  0.2825,</span><br><span class="line">          0.0227,  0.2057, -0.0854, -0.0104,  0.0594, -0.2546, -0.1229, -0.0106,</span><br><span class="line">          0.1834,  0.0267, -0.0300, -0.2368,  0.1866,  0.1788,  0.1391, -0.0533,</span><br><span class="line">         -0.0224, -0.0218,  0.2190,  0.1085, -0.1313,  0.0817, -0.2850, -0.0282,</span><br><span class="line">         -0.0171,  0.2833,  0.1667, -0.0544,  0.2196,  0.0851,  0.2685, -0.0164,</span><br><span class="line">         -0.1700,  0.0058,  0.1255, -0.0930,  0.1692, -0.1948, -0.1032, -0.1335,</span><br><span class="line">          0.1251,  0.0843]], grad_fn=&lt;MmBackward&gt;)</span><br></pre></td></tr></table></figure>
<p>可以看到得到的tensor形状相同。</p>
<h2 id="特别注意"><a class="markdownIt-Anchor" href="#特别注意"></a> 特别注意</h2>
<hr />
<p>在实装中看到过这样的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TokenEmbedding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size: <span class="built_in">int</span>, emb_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TokenEmbedding, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, emb_size)</span><br><span class="line">        self.emb_size = emb_size</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, tokens: Tensor</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.embedding(tokens.long()) * math.sqrt(self.emb_size)</span><br></pre></td></tr></table></figure>
<p>重点看<code>forward</code>部分，最后乘上了一个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msqrt><mrow><mi>e</mi><mi>m</mi><mi>b</mi><mi mathvariant="normal">_</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi></mrow></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{emb\_size}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.24em;vertical-align:-0.3627800000000001em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8772199999999999em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mord mathdefault">e</span><span class="mord mathdefault">m</span><span class="mord mathdefault">b</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">s</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mord mathdefault">e</span></span></span><span style="top:-2.8372200000000003em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119c34,79.3,68.167,
158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120c340,-704.7,510.7,-1060.3,512,-1067
c4.7,-7.3,11,-11,19,-11H40000v40H1012.3s-271.3,567,-271.3,567c-38.7,80.7,-84,
175,-136,283c-52,108,-89.167,185.3,-111.5,232c-22.3,46.7,-33.8,70.3,-34.5,71
c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1s-109,-253,-109,-253c-72.7,-168,-109.3,
-252,-110,-252c-10.7,8,-22,16.7,-34,26c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26
s76,-59,76,-59s76,-60,76,-60z M1001 80H40000v40H1012z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3627800000000001em;"><span></span></span></span></span></span></span></span></span>，在实装中这么做的原因实际上是原始论文中这么写到的：</p>
<blockquote>
<p><strong>3.4 Embeddings and Softmax</strong></p>
<p>Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension 𝑑model. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [24]. In the embedding layers, we multiply those weights by √𝑑model</p>
</blockquote>
<p>但是到底为什么要这么做也没有真正地说明，经过我搜寻过后看到有人认为也许因为Embedding Layer的权重是经过<code>xavier</code>初始化的，标准差为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><msqrt><mrow><mi>e</mi><mi>m</mi><mi>b</mi><mi mathvariant="normal">_</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi></mrow></msqrt></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{1}{\sqrt{emb\_size}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.45144em;vertical-align:-1.13em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.23278em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8772199999999999em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mord mathdefault">e</span><span class="mord mathdefault">m</span><span class="mord mathdefault">b</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">s</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mord mathdefault">e</span></span></span><span style="top:-2.8372200000000003em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119c34,79.3,68.167,
158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120c340,-704.7,510.7,-1060.3,512,-1067
c4.7,-7.3,11,-11,19,-11H40000v40H1012.3s-271.3,567,-271.3,567c-38.7,80.7,-84,
175,-136,283c-52,108,-89.167,185.3,-111.5,232c-22.3,46.7,-33.8,70.3,-34.5,71
c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1s-109,-253,-109,-253c-72.7,-168,-109.3,
-252,-110,-252c-10.7,8,-22,16.7,-34,26c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26
s76,-59,76,-59s76,-60,76,-60z M1001 80H40000v40H1012z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3627800000000001em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.13em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>然后通过这个操作，把方差归到1，但是我查了<code>PyTroch</code>的文档默认的初始化并不是<code>xavier</code>初始化：</p>
<blockquote>
<p><strong>~Embedding.weight</strong> (<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(0, 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.14736em;">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></p>
</blockquote>
<p>所以需要注意一下实际的代码中是否有类似以下额外做初始化的内容：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> transformer.parameters():</span><br><span class="line">    <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">        nn.init.xavier_uniform_(p)</span><br></pre></td></tr></table></figure>
<p>然而是否真的是这个原因似乎还没有定论的样子，可以参见<a target="_blank" rel="noopener" href="https://datascience.stackexchange.com/questions/87906/transformer-model-why-are-word-embeddings-scaled-before-adding-positional-encod">Transformer model: Why are word embeddings scaled before adding positional encodings?</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Kyox24</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://foggyworld.life/posts/2a127db8/">https://foggyworld.life/posts/2a127db8/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://foggyworld.life" target="_blank">Foggy World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/PyTorch/">PyTorch</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/kyox24/PicBed/MEMO_PY_EM.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/dba5fd70/"><img class="prev-cover" data-lazy-src="https://cdn.jsdelivr.net/gh/kyox24/PicBed/MEMO_ONEHOT.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">one-hot编码和batch normalization的关系</div></div></a></div><div class="next-post pull-right"><a href="/posts/5febf64f/"><img class="next-cover" data-lazy-src="https://cdn.jsdelivr.net/gh/kyox24/PicBed/MEMO_TEMPERR.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">记一次奇怪的Template Render Error</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/dba5fd70/" title="one-hot编码和batch normalization的关系"><img class="cover" data-lazy-src="https://cdn.jsdelivr.net/gh/kyox24/PicBed/MEMO_ONEHOT.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-05-12</div><div class="title">one-hot编码和batch normalization的关系</div></div></a></div><div><a href="/posts/c9f13a0f/" title="TOAD-GAN在Google Colab训练时关于wandb输出的踩坑记录"><img class="cover" data-lazy-src="https://cdn.jsdelivr.net/gh/kyox24/PicBed/MEMO_toadganwandb.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-05-18</div><div class="title">TOAD-GAN在Google Colab训练时关于wandb输出的踩坑记录</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" data-lazy-src="https://cdn.jsdelivr.net/gh/kyox24/PicBed/头像.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Kyox24</div><div class="author-info__description">飞蛾扑火</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/kyox24"><i class="fab fa-github"></i><span>Github</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/kyox24" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:kyo19951024@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://weibo.com/p/1005051882165114" target="_blank" title="Weibo"><i class="fab fa-weibo"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">缓慢搭建中</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text"> 前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFembedding"><span class="toc-number">2.</span> <span class="toc-text"> 什么是Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E4%B8%ADnnembedding%E7%9A%84%E7%94%A8%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text"> PyTorch中nn.Embedding的用法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E4%B8%AD%E4%BD%BF%E7%94%A8%E4%BA%86nnembedding%E4%B9%8B%E5%90%8E%E8%AE%AD%E7%BB%83%E6%97%B6%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88"><span class="toc-number">4.</span> <span class="toc-text"> PyTorch中使用了nn.Embedding之后训练时发生了什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%88%AB%E6%B3%A8%E6%84%8F"><span class="toc-number">5.</span> <span class="toc-text"> 特别注意</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/43392113/" title="Adversarial Reinforcement Learning for Procedural Content Generation | 论文笔记"><img data-lazy-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Adversarial Reinforcement Learning for Procedural Content Generation | 论文笔记"/></a><div class="content"><a class="title" href="/posts/43392113/" title="Adversarial Reinforcement Learning for Procedural Content Generation | 论文笔记">Adversarial Reinforcement Learning for Procedural Content Generation | 论文笔记</a><time datetime="2021-05-25T07:58:48.000Z" title="发表于 2021-05-25 16:58:48">2021-05-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/c9f13a0f/" title="TOAD-GAN在Google Colab训练时关于wandb输出的踩坑记录"><img data-lazy-src="https://cdn.jsdelivr.net/gh/kyox24/PicBed/MEMO_toadganwandb.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="TOAD-GAN在Google Colab训练时关于wandb输出的踩坑记录"/></a><div class="content"><a class="title" href="/posts/c9f13a0f/" title="TOAD-GAN在Google Colab训练时关于wandb输出的踩坑记录">TOAD-GAN在Google Colab训练时关于wandb输出的踩坑记录</a><time datetime="2021-05-17T15:45:00.000Z" title="发表于 2021-05-18 00:45:00">2021-05-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/29b10db/" title="SinGAN：Learning a Generative Model from a Single Natural Image | 论文笔记"><img data-lazy-src="https://cdn.jsdelivr.net/gh/kyox24/PicBed/LN_SINGAN.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="SinGAN：Learning a Generative Model from a Single Natural Image | 论文笔记"/></a><div class="content"><a class="title" href="/posts/29b10db/" title="SinGAN：Learning a Generative Model from a Single Natural Image | 论文笔记">SinGAN：Learning a Generative Model from a Single Natural Image | 论文笔记</a><time datetime="2021-05-16T14:56:44.000Z" title="发表于 2021-05-16 23:56:44">2021-05-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/dba5fd70/" title="one-hot编码和batch normalization的关系"><img data-lazy-src="https://cdn.jsdelivr.net/gh/kyox24/PicBed/MEMO_ONEHOT.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="one-hot编码和batch normalization的关系"/></a><div class="content"><a class="title" href="/posts/dba5fd70/" title="one-hot编码和batch normalization的关系">one-hot编码和batch normalization的关系</a><time datetime="2021-05-12T02:12:49.000Z" title="发表于 2021-05-12 11:12:49">2021-05-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/2a127db8/" title="PyTorch中nn.Embedding相关备忘"><img data-lazy-src="https://cdn.jsdelivr.net/gh/kyox24/PicBed/MEMO_PY_EM.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="PyTorch中nn.Embedding相关备忘"/></a><div class="content"><a class="title" href="/posts/2a127db8/" title="PyTorch中nn.Embedding相关备忘">PyTorch中nn.Embedding相关备忘</a><time datetime="2021-05-11T14:54:03.000Z" title="发表于 2021-05-11 23:54:03">2021-05-11</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Kyox24</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'blog-comment-7ggp4lbmaeda677f',
      region: ''
    }, null))
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-comment-7ggp4lbmaeda677f',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>